{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "This notebook was made in order to give a brief introduction to hyperparameter tuning. A comparison between the most basic algorithms is shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Preprocessing\n",
    "# x_train = np.expand_dims(x_train, axis=3).astype('float32')/255.0\n",
    "# x_test = np.expand_dims(x_test, axis=3)/255.0\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_idx =  np.random.randint(0, len(x_train)-1)\n",
    "# plt.imshow(x_train[random_sample_idx], cmap='gray')\n",
    "plt.imshow(x_train[random_sample_idx])\n",
    "plt.show()\n",
    "print(f'label: {y_train[random_sample_idx]}')\n",
    "print(f'input shape: {len(x_train[0])} by {len(x_train[0][0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "A simple classification model based on Convolutional Neuronal Networks will be used. It consists of three Convolutional layers with ReLU activations, MaxPooling and dropout regularization for encoding the image, and a two-layered Fully-Connected Network for classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Environment\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D,Dense, Dropout, Flatten, MaxPooling2D\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "INPUT_SHAPE = x_train[0].shape\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 25\n",
    "SEED = 37\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "tf.random.set_seed(SEED)\n",
    "model = keras.Sequential()\n",
    "model.add(\n",
    "    Conv2D(\n",
    "        filters=8,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        input_shape=INPUT_SHAPE\n",
    "    )\n",
    ")\n",
    "model.add(Conv2D(16, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(1e-3),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "model.fit(x_train, y_train, epochs=EPOCHS, validation_split=0.1)\n",
    "end_t = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss, base_accuracy = model.evaluate(x_test, y_test)\n",
    "base_elapsed_time = end_t - start_t\n",
    "print(f\"Elapsed time (s): {base_elapsed_time:0.2f} (s)\")\n",
    "print(f'Baseline loss: {base_loss:0.3f}, accuracy: {base_accuracy:0.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Hyperparameter Tuning tools\n",
    "\n",
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "class CNNHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=16,\n",
    "                kernel_size=3,\n",
    "                activation=\"relu\",\n",
    "                input_shape=self.input_shape,\n",
    "            )\n",
    "        )\n",
    "        model.add(Conv2D(16, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    \"dropout_1\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Conv2D(\n",
    "                filters=hp.Choice(\"num_filters\", values=[16, 32, 64, 128], default=32,),\n",
    "                activation=\"relu\",\n",
    "                kernel_size=3,\n",
    "            )\n",
    "        )\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    \"dropout_2\", min_value=0.0, max_value=0.5, default=0.25, step=0.05,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(Flatten())\n",
    "        model.add(\n",
    "            Dense(\n",
    "                units=hp.Int(\n",
    "                    \"units\", min_value=32, max_value=512, step=16, default=128\n",
    "                ),\n",
    "                activation=hp.Choice(\n",
    "                    \"dense_activation\",\n",
    "                    values=[\"relu\", \"tanh\", \"sigmoid\"],\n",
    "                    default=\"relu\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            Dropout(\n",
    "                rate=hp.Float(\n",
    "                    \"dropout_3\", min_value=0.0, max_value=0.5, default=0.25, step=0.05\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        model.add(Dense(self.num_classes, activation=\"softmax\"))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    \"learning_rate\",\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling=\"LOG\",\n",
    "                    default=1e-3,\n",
    "                )\n",
    "            ),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental Variables\n",
    "from pathlib import Path\n",
    "from kerastuner.tuners import BayesianOptimization, Hyperband, RandomSearch\n",
    "\n",
    "model = CNNHyperModel(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES)\n",
    "\n",
    "output_dir = Path(\"./output/cifar10/\")\n",
    "project_name = \"simple_cnn_model_tuning\"\n",
    "HYPERBAND_MAX_EPOCHS = 25\n",
    "MAX_TRIALS = 20\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "BAYESIAN_NUM_INITIAL_POINTS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuner definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuners = [\n",
    "    RandomSearch(\n",
    "        model,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=SEED,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "        directory=f\"{output_dir}_random_search\",\n",
    "        project_name=project_name,\n",
    "    ),\n",
    "    Hyperband(\n",
    "        model,\n",
    "        max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "        objective=\"val_accuracy\",\n",
    "        seed=SEED,\n",
    "        executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "        directory=f\"{output_dir}_hyperband\",\n",
    "        project_name=project_name,\n",
    "    ),\n",
    "    BayesianOptimization(\n",
    "        model,\n",
    "        objective='val_accuracy',\n",
    "        seed=SEED,\n",
    "        num_initial_points=BAYESIAN_NUM_INITIAL_POINTS,\n",
    "        max_trials=MAX_TRIALS,\n",
    "        directory=f\"{output_dir}_bayesian\",\n",
    "        project_name=project_name\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuner Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_workflow(tuner, x_train, y_train, x_test, y_test):\n",
    "    tuner.search_space_summary()\n",
    "    search_start = time.time()\n",
    "    tuner.search(x_train, y_train, epochs=EPOCHS, validation_split=0.1)\n",
    "    search_end = time.time()\n",
    "    elapsed_time = search_end - search_start\n",
    "\n",
    "    # Show a summary of the search\n",
    "    tuner.results_summary()\n",
    "\n",
    "    # Retrieve the best model.\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Evaluate the best model.\n",
    "    loss, accuracy = best_model.evaluate(x_test, y_test)\n",
    "    return elapsed_time, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[base_elapsed_time, base_loss, base_accuracy]]\n",
    "for tuner in tuners:\n",
    "    elapsed_time, loss, accuracy = tuner_workflow(\n",
    "        tuner, x_train, y_train, x_test, y_test\n",
    "    )\n",
    "    results.append([elapsed_time, loss, accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import pickle\n",
    "with open('./output/tuners.pk', 'wb') as f:\n",
    "    pickle.dump(tuners, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Driven Hyperparameter Tuning tools\n",
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypermodel_func(hyperparams, trial):\n",
    "    tf.random.set_seed(SEED)\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        Conv2D(\n",
    "            filters=8,\n",
    "            kernel_size=3,\n",
    "            activation='relu',\n",
    "            input_shape=INPUT_SHAPE\n",
    "        )\n",
    "    )\n",
    "    model.add(Conv2D(16, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(rate=hyperparams['dropout_1'][trial]))\n",
    "    model.add(Conv2D(filters=hyperparams['num_filters'][trial], kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(rate=hyperparams['dropout_2'][trial]))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=hyperparams['units'][trial], \n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(rate=hyperparams['dropout_3'][trial]))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    model.compile(\n",
    "            optimizer=keras.optimizers.Adam(hyperparams['learning_rate'][trial]),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "    history = model.fit(x_train, y_train, epochs=EPOCHS, validation_split=0.1)\n",
    "    return model, history          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisaratuners import aisara_keras_tuner as akt\n",
    "\n",
    "hyperparams = akt.Hp()\n",
    "hyperparams.numrange(name='dropout_1', min=0.0, max=0.5)\n",
    "hyperparams.numrange(name='num_filters', min=16, max=128)\n",
    "hyperparams.numrange(name='dropout_2', min=0.0, max=0.5)\n",
    "hyperparams.numrange(name='units', min=32, max=512)\n",
    "hyperparams.numrange(name='dropout_3', min=0.0, max=0.5)\n",
    "hyperparams.numrange(name='learning_rate', min=1e-4, max=1e-2, type='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuner configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = akt.HpOptimization(\n",
    "    hyperparams, \n",
    "    hypermodel_func, \n",
    "    ['val_accuracy', 'val_loss'], \n",
    "    ['max', 'min'], \n",
    "    num_trials=5, \n",
    "    rounds=3,\n",
    "    mode='p',\n",
    "    aisara_seed='fixed'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_t = time.time()\n",
    "tuner.run_opti()\n",
    "end_t = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Tuner performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_best_model = tuner.best_model\n",
    "ai_loss, ai_accuracy = model.evaluate(x_test, y_test)\n",
    "ai_elapsed_time = end_t - start_t\n",
    "print(f\"Elapsed time (s): {ai_elapsed_time:0.2f} (s)\")\n",
    "print(f'AI Tuner Best model loss: {ai_loss:0.3f}, accuracy: {ai_accuracy:0.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
